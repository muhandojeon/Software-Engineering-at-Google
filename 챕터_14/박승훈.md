## 더 큰 테스트

- 느릴 수 있다.
- 밀폐되지 않을 수 있다. 다른 테스트나 최종 사용자와 자원 및 트래픽을 공유
- 비결정적일 수 있다.

- 단위 테스트는 개별 함수, 객체, 모듈에 대한 확신을 심어준다.
- 더 큰 테스트들은 시스템 '전체'가 의도대로 동작한다는 확신을 더해주는 역할

### 충실성

- 더 큰 테스트의 핵심은 **성능과 충실성 사이에서 가장 적합한 지점을 찾아내는 것**
- 프로덕션 환경에서 수집한 데이터라면 실제를 훨씬 잘 반영할 것

### 단위 테스트가 손대기 어려운 영역

단위 테스트로는 위험 요인을 충분히 해소하기 어려운 대표적인 영역 다섯 가지

#### 부정확한 테스트 대역

- 무겁고 테스트하기 어려운 의존성을 제거하기 위해 테스트 대역 사용
- 이러면 실제와 대역의 동작이 일치하지 않을 가능성이 높다.

#### 설정 문제

- 단위 테스트가 다루는 바이너리는 단독으로 실행할 수 없다.
- 배포 설정이나 시작 스크립트 등 설정 파일의 호환성 문제를 검증할 수 없다.
- 구글에서 심각한 서비스 장애를 일으키는 가장 주된 원인은 설정 변경
- 설정은 설정 전용 언어로 작성되는 게 일반

> 설정 변경이 구글에서 심각한 서비스 장애를 일으킨다는 게 의외네요.

#### 과부하시 나타는 문제

- 상당한 양의 트래픽은 단위 테스트 모델에 녹이기 어렵다.

#### 예기치 못한 동작, 입력, 부작용

- 단위 테스트는 작성자의 상상력에 갇힌다.
- 작성자가 예상할 수 있는 행위와 입력에 한해 테스트되기 쉽다.
- [하이럼의 법칙](https://www.hyrumslaw.com/) : 실제 사용자는 명시된 약속뿐 아니라 눈에 보이는 모든 것을 자유롭게 이용 가능

### 창발적 해위와 '진공 효과'

- 단위 테스트가 다루는 범위는 제한적
- 단위 테스트는 빠르고 안정적인 게 중점이므로 외부세계와의 연결과 그로 인한 혼돈을 의도적으로 배제
- 마치 '진공 상태를 가정'하고 문제를 푸는 것. 실제와는 다르게.

> 진공 상태를 가정하고 이론 물리학을 푼다는 개념이 마음에 드네요. 좋은 비유라고 생각합니다.

### 더 큰 테스트를 만들지 않는 이유

- 단위 테스트의 장점인 `높은 신뢰성`, `빠른 속도`, `높은 확장성`
- 이 밖에도 극복해야 하는 과제가 2가지 더 있다.
- `소유권 문제` : 유지보수는 누가, 테스트가 실패하면 누가 책임질 것인가?
- `표준화, 표준화 부족` : 실행 방식이 너무 다양하여 인프라의 지원을 받지 못한다.

## 더 큰 테스트 @ 구글

### 더 큰 테스트와 수명

- 테스트란 기대 수명에 상관없이 의미가 있는 활동
- 어떤 테스트가 가장 적합한지는 코드의 기대 수명에 따라 달라진다.
- 큰 테스트일수록 수명이 더 긴 소프트웨어에 유용하다.
- 수명이 길어질수록 주 관심사가 테스트의 유지보수로 옮겨간다.
- 건강한 상태를 오래 유지하는 '핵심'은 **개발 시작 후 며칠 안으로 단위 테스트를 만들어 테스트 피라미드를 쌓는 것**

> 얼마 전에 supabase에 업로드하는 파일의 validation 규칙을 만족하기 위한 파일명 치환 함수의 테스트 파일을 작성했는데요, test case를 크게 만족하지 않아서 애를 먹었어요. AI한테 테스트 케이스를 만들어달라고 했는데도 만족스럽지 않은 결과물을 주기도 했고요. 그러다 이 함수 하나에 씨름하고 싶지 않아서 많은 테스트 케이스를 지우고 넘어갔는데.. 아직은 제 부족함이 크게 느껴지네요ㅜㅜ

### 구글 규모에서의 더 큰 테스트

- 큰 테스트에서는 원하는 규모에서 잘 작동하면서도 충실성이 상당히 높게 구현하는 게 관건
- 큰 초거대 테스트 대신 `연쇄 테스트`를 활용하라. 작은 통합 테스트의 연결.

## 큰 테스트를 진행하는 흐름

### 테스트 대상 시스템(SUT)

- 대규모 테스트의 핵심
- 대규모 테스트에서의 SUT는 대체로 하나 이상의 독립된 프로세스에서 수행
- SUT의 형태 결정 요소: 밀폐성과 충실성
  - 밀폐성이 높은 SUT는 동시성 문제나 불규칙한 인프라로부터 영향을 적게 받는다.
  - 충실성이 높은 SUT는 프로덕션 버전과 유사한 바이너리로 구성된다.
  - SUT의 충돌에 따라 단일 프로세스, 단일 머신, 다중 머신, 공유 환경, 하이브리드 등 다양한 형상을 띈다.

#### 밀폐된 SUT의 이점

- 프로덕션 환경에서의 테스트는 실제 운영중인 시스템을 이용한다.
- 환경에 릴리즈되는 시점을 테스트 수행자가 직접 통제하지 못한다. SUT가 너무 준비될 수 있다.
- 대안으로는 거대한 공유 스테이징 환경을 만드는 것이 있다. 하지만 공유 환경에 반영된 이후 테스트할 수 있다는 한계는 여전하다.
- 최적의 대안으로는 클라우드에서 격리된 영역을 만들어내거나 머신을 밀폐할 수 있는 환경을 구축하고 그 안에 SUT를 배포한다.

#### 문제 경계에서 SUT 크기 줄이기

- 사용자 인터페이스(UI) 테스트는 신뢰하기 어렵고 비용이 많이 든다. 테스트를 UI/API 경계에서 나누는 것이 좋다.
- 서드파티 API를 직접 사용하는 자동 테스트는 권장하지 않는다. 테스트를 분할하는 기점으로 삼자.

#### 기록/재생 프록시

- SUT가 의존하지만 보조적인 서비스라면 테스트 대역으로 대체할 수 있다.
- 구글에서는 `고객 주도 계약` 테스트용 프레임워크를 사용한다. 지켜야 할 계약(명세)을 정의하고, 이를 토대로 자동화된 테스트를 만드는 방식
- 큰 테스트를 실행해 외부 서비스들과의 트래픽을 `기록`하고, 이를 더 작은 테스트를 수행할 때 `재생`하는 것

> 실제 트래픽을 더 작은 테스트를 위해 재사용한다는 개념이 재밌네요.

### 테스트 데이터

대규모 테스트에는 두 가지 데이터가 필요하다.

- `시드 데이터`: 테스트 개시 시점의 SUT 상태를 반영하여 SUT를 사전 초기화해주는 데이터
- `테스트 트래픽`: 테스트 수행 과정에서 SUT로 보내는 데이터

단위 테스트에 비해 SUT의 상태를 테스트 초전에 초기화하는 작업은 대체로 훨씬 복잡하다.

- 도메인 데이터: 환경 구성용 데이터가 미리 채워져 있어야 한다.
- 현실적인 기준선: 현실적인 크기의 데이터셋과 관계가 맺어져 있어야 한다.
- 데이터 기록 API: 데이터를 기록하는 API가 복잡할 수도.

데이터는 다양한 방식으로 생성할 수 있다.

- 손수 가공한 데이터: 작은 테스트에서 가능
- 복사한 데이터: (주로 프로덕션 시스템의) 데이터를 복사
- 샘플링한 데이터: `스마트 샘플링` - 테스트 커버리지를 극대화하는 최소한의 데이터만 추출하여 사용하는 기술

### 검증

검증 방식도 여러 가지이다.

- 수동 검증: 사람이 SUT와 직접 상호작용하며 확인하는 방식. 선형으로 확장되지 않는다.
- 단정문: 단위테스트처럼 시스템이 의도된 대로 동작하는지 검사하는 검증 방식
- A/B 비교: 두 벌의 SUT를 구동시켜 똑같은 데이터를 보낸 다음 결과를 비교하는 검증 방식

## 더 큰 테스트 유형

테스트 유형별로 어떤 위험을 완화해주는지, 작성 • 유지 • 디버그에 드는 노력은 어느 정도인지, 실행 비용은 얼마나 되는지 등의 특성이 달라진다.

### 하나 혹은 상호작용하는 둘 이상의 바이너리 기능 테스트

- 이런 유형의 테스트가 지니는 특성
  - SUT: 밀폐된 단일 머신 혹은 격리된 클라우드에 배포
  - 데이터: 수동 생성
  - 검증 방식: 단정문

### 브라우저와 기기 테스트

- 웹 UI나 모바일 애플리케이션 테스트
- 서드파티 입장이 되어 FE를 통해 애플리케이션을 이용하는 테스트는 커버리지를 높이는 또 다른 수단

### 성능, 부하, 스트레스 테스트

- 이런 유형의 테스트가 지니는 특성
  - SUT: 격리된 클라우드에 배포
  - 데이터: 수동 생성 혹은 프로덕션 환경에서 복사
  - 검증 방식: 차이 비교(성능 지표)

정의상 주로 바이너리 검증용에 쓰이는 다중 스레드 테스트

### 배포 설정 테스트

- 이런 유형의 테스트가 지니는 특성
  - SUT: 밀폐된 단일 머신 혹은 격리된 클라우드에 배포
  - 데이터: 없음
  - 검증 방식: 단정문(비정상 종료는 하지 않음)
- 결함의 원인이 소스 코드가 아니라 데이터 파일, DB, 옵션 등의 설정에 있는 경우

### 탐색적 테스팅

- 새로운 사용자 시나리오를 시도해가며 의문스러운 동작을 찾는 **수동 테스트**
- 이런 유형의 테스트가 지니는 특성
  - SUT: 프로덕션 혹은 공유 스테이징 환경에 배포
  - 데이터: 프로덕션에서 수집 혹은 알려진 테스트 시나리오 데이터
  - 검증 방식: 수동
- 예상치 못한 동작과 부작용을 발견해낼 수 있어 유용
- 한계: 수동 테스트라 선형적으로 확장되지 않는다. 발견한 모든 결함은 자동 테스트로 만들어야 한다.
- 버그파티: 관련된 모든 사람이 제품을 수동으로 테스트한다.

### A/B 차이 회귀 테스트

- 구버전 제품과 신버전 제품의 공개 API로 트래픽을 보내 둘의 반응이 어떤지 비교
- 이런 유형의 테스트가 지니는 특성
  - SUT: 두 개의 격리된 클라우드에 배포
  - 데이터: 대체로 프로덕션 환경에서 복사한 혹은 샘플링한 데이터
  - 검증 방식: A/B 차이 비교
- 출시된 시스템에서 예상치 못한 부작용을 찾아낼 수 있는 저렴하면서도 자동화 가능한 방법
- 한계
  - 인가: 발견된 차이를 사람이 개입해서 해석
  - 노이즈: 결과에 차이를 만드는 것이 노이즈 때문은 아닌지 조사 필요
  - 커버리지: 의미 있는 트래픽을 생성하기 어려울 수도.
  - 설정(setup): 설정이 복잡해진다.

### 프로버와 카나리 분석

- 프로덕션 환경 자체가 건강함을 보장하는 수단
- 이런 유형의 테스트가 지니는 특성
  - SUT: 프로덕션에 배포
  - 데이터: 프로덕션에서 수집
  - 검증 방식: 단정문과 (지표상의) A/B 차이
- 프로버(Prober): 프로덕션 환경을 대상으로 단정문을 수행하는 기능 테스트
- 프로버는 서비스중인 시스템 모두에 수행해야 한다.
- 카나리 분석(Canary analysis): 신버전을 프로덕션 환경에 언제 배포할지가 주된 관심사
- 한계: 프로덕션 환경에서 이루어지므로 이미 최종 사용자에게 문제의 영향을 주고 있음

### 재해 복구와 카오스 엔지니어링

- 시스템이 예기치 못한 변경이나 실패에도 얼마나 굳건히 대응하는가를 확인하는 테스트
- 이런 유형의 테스트가 지니는 특성
  - SUT: 프로덕션에 배포
  - 데이터: 프로덕션에서 수집 혹은 사용자가 제작(결함 주입)
  - 검증 방식: 수동 및 (지표상의) A/B 차이
- 카오스 엔지니어링: 시스템에 꾸준히 결함을 심어서 무슨 일이 벌어지는지 관찰하는 테스트
  - 시스템이 안정적이고 신뢰할 수 있다는 가정을 깨서 시스템에 복원력이 갖춰지도록 돕는 게 목적

> 이번 부분은 조금 재미있는데요, 구글의 시뮬레이션에서 구글의 본사가 지진으로 완전히 무너져 의사 결정권자와의 단절 속에서 서비스가 어떻게 운영될지를 알아보는 거라던지, 기타 전지구적 재해를 가정한다는 사실이 흥미로웠습니다. 역시 구글이네요. 또한 매너리즘에 빠지는 것을 방지하기 위해 일부러 결함을 심는 카오스 엔지니어링의 존재도 잘 몰랐는데, 유용하면서도 막상 당하면 짜증은 날 것 같은 제도 같습니다ㅋㅋㅋ

### 사용자 평가

- 이런 유형의 테스트가 지니는 특성
  - SUT: 프로덕션에 배포
  - 데이터: 프로덕션에서 수집
  - 검증 방식: 수동 및 (지표상의) A/B 차이
- 개밥 주기(dogfooding, 사내 시험 적용): 직원들을 대상으로 새로운 기능을 경험하게 하기
- 실험(experimentation): 새로운 기능을 사용자에게 알리지 않고 제공한 뒤, 원하는 지표를 비교
- 평가자 감정: 변화된 결과를 인간 평가자(rater)들에게 피드백 받기

## 큰 테스트와 개발자 워크플로

- 대규모 테스트는 누가 작성할까? 정상적으로 수행되도록 유지하기 위해서는 정말 힘들텐데?
- '개발자의 고통과 늦어지는 변경 반영 시간'과 '지속적 빌드의 신뢰성' 사이에서 절충점을 찾아야 한다.

### 큰 테스트 작성하기

- 명확한 라이브러리, 문서자료, 예시 코드를 참조하라.
- A/B 테스트가 인기인 이유 중 하나는 검증 단계 관리에서 사람이 신경을 덜 써도 되기 때문

### 큰 테스트 수행하기

- 가능하면 더 큰 테스트들을 엔지니어에게 친숙한 방식으로 수행하자.

#### 테스트 속도 개선하기

- 테스트 범위를 줄이거나 작은 테스트의 병렬 수행한다.
- 큰 테스트는 스레드를 마음껏 쓸 수 있다.
  - ms 단위로 polling하며 작업을 확인
  - 이벤트 완료를 알려주는 알림 시스템 등록
- 내부 시스템 타임아웃과 지연 낮추기
- 테스트 빌드 시간 최적화

#### 불규칙한 결과에서 벗어나기

- 불규칙한 결과는 단위 테스트는 물론, 더 큰 테스트에서는 활용 자체를 못하게 한다.
- 불규칙성을 최소화하려면 테스트 범위를 줄여야 한다.
- 테스트 타임아웃과 사용자의 감내 수준의 합의점을 찾는다.

#### 이해되는 테스트 만들기

- 테스트 엔지니어가 이해할 수 없는 결과를 낳는 테스트는 개발자 워크플로에 통합하기가 특히 더 어렵다.
- 무엇이 실패했는지 명확히 알려주자. 기대 결과와 실제 결과를 명확히 설명하자.
- 최소한의 노력으로 근본 원인을 찾을 수 있도록 하자.
- 지원 정보 및 연락처 정보를 제공하자. 지원 책임자와 쉽게 연락할 수 있도록 하자.

### 큰 테스트의 소유권

- 큰 테스트에는 반드시 소유자가 문서로 기록되어 있어야 한다.
- 소유자: 테스트가 변경될 때 검토하고 테스트가 실패할 때 지원할 수 있는 사람.
- 특정 프로젝트의 통합 테스트라면 프로젝트 리드가 맡는다.
- 기능에 중점을 둔 테스트라면 기능의 소유자가 테스트도 소유한다.
- 소유자가 누구든 테스트이 전반적인 건실성을 보장하고 유지보수를 지원하고 보상을 줄 수 있는 권한을 모두 갖춘 사람이어야 한다.
